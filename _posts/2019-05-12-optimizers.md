---
toc: true
comments: true
hide : true
layout: post
description: Why sample mean is a good way to estimate true mean.
categories: [probability, maths ]
title: Optimizers
---

Optimizers are needed to find the optimal solution for the given task. Optimizers associate themselves with cost function and model parameters together by updating the model. i.e. when you want to identify weights which minimize your mean squared error in linear regression, you need to use some function to find parameters such that mean squared error is minimum, this function is called optimizer. So you use optimizer function to reach global minima with respect to the cost function. 

Types of optimizer: 
1. Gradient Descent 
2. Momentum 
3. d
4. 

Stochastic Gradient Descent, or SGD for short, is one of the simplest optimization algorithms. It uses just one static learning rate for all parameters during the entire training phase.
The static learning rate does not imply an equal update after every minibatch. As the optimizers approach an (sub)optimal value, their gradients start to decrease.

Momentum: Momentum helps accelerate 

https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3



